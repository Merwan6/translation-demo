{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57e9971e",
   "metadata": {},
   "source": [
    "# üß™ Projet : Application de Traduction Multimod√®le avec √âvaluation\n",
    "\n",
    "## üéØ Objectif\n",
    "\n",
    "- Cr√©er une application de traduction multilingue utilisant plusieurs mod√®les de LLM.\n",
    "- Comparer leurs performances avec des m√©triques standard.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ T√¢ches √† r√©aliser\n",
    "\n",
    "1. üîç **Rechercher un jeu de donn√©es de traduction de r√©f√©rence**  \n",
    "   Le dataset doit contenir :\n",
    "   - du **texte original**\n",
    "   - une **traduction de r√©f√©rence**  \n",
    "   üëâ Sources possibles : Hugging Face, Kaggle, GitHub, etc.\n",
    "\n",
    "2. üß† **Cr√©er une application de traduction d√©mo**  \n",
    "   Utiliser :\n",
    "   - **Deux mod√®les LLM de Ollama**\n",
    "   - **Deux mod√®les LLM de Hugging Face**  \n",
    "   üëâ Langue au choix selon le dataset (ex : anglais ‚Üí fran√ßais)\n",
    "\n",
    "3. üéõÔ∏è **Utiliser diff√©rents niveaux de temp√©rature** pour la g√©n√©ration :\n",
    "   - `0`, `0.2`, `0.5`, `0.8`, `1.0`\n",
    "\n",
    "4. üìä **√âvaluer les r√©sultats** de traduction en utilisant :\n",
    "   - **BLEU**\n",
    "   - **ROUGE**\n",
    "\n",
    "5. üíª **D√©ployer une application interactive avec Gradio**  \n",
    "   - L‚Äôapplication doit tourner en local\n",
    "\n",
    "6. üöÄ **Publier le projet sur GitHub** :\n",
    "   - Code complet + `requirements.txt`\n",
    "   - Rapport de comparaison des performances des mod√®les\n",
    "   - Vid√©o d√©monstration de l‚Äôapplication\n",
    "   - Lien vers le repo √† partager\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Technologies sugg√©r√©es\n",
    "\n",
    "- [ü§ó Transformers ‚Äì Hugging Face](https://huggingface.co/transformers/)\n",
    "- [Ollama ‚Äì LLMs locaux](https://ollama.com/)\n",
    "- [Gradio ‚Äì Interface utilisateur](https://www.gradio.app/)\n",
    "- `nltk`, `rouge_score` pour l'√©valuation\n",
    "- Python, Jupyter, GitHub\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073a180",
   "metadata": {},
   "source": [
    "# 0) Importations des biblioth√®ques + chemin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4473382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer, MBartForConditionalGeneration, MBart50Tokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "from sacrebleu import corpus_bleu\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672979d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OLLAMA_PATH = os.getenv(\"OLLAMA_PATH\")\n",
    "if not OLLAMA_PATH:\n",
    "    raise EnvironmentError(\"OLLAMA_PATH n'est pas d√©fini dans le fichier .env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b322494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollama version is 0.9.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "OLLAMA_PATH = r\"C:\\Users\\merwa\\AppData\\Local\\Programs\\Ollama\\ollama.exe\"\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([OLLAMA_PATH, \"--version\"], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b371f",
   "metadata": {},
   "source": [
    "# 1) Chargement du dataset wmt14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ce8bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee0eddd88484af1a5aa9a1095869400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebbd6a1e4194dad9c73e4c10c8b3f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Charger dataset WMT14 fr-en\n",
    "dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "\n",
    "def get_en_fr_pairs(dataset, split='test', limit=10):\n",
    "    subset = dataset[split].select(range(limit)) \n",
    "    sources = [item['translation']['en'] for item in subset]\n",
    "    references = [item['translation']['fr'] for item in subset]\n",
    "    return sources, references\n",
    "\n",
    "sources, references = get_en_fr_pairs(dataset, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1b7e4",
   "metadata": {},
   "source": [
    "# 2) Choisir et utiliser 2 mod√®les Hugging Face + 2 mod√®les Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc7e5f",
   "metadata": {},
   "source": [
    "| Mod√®le                               | Taille (param√®tres) | Usage principal         | Notes                         |\n",
    "| ------------------------------------ | ------------------- | ----------------------- | ----------------------------- |\n",
    "| Helsinki-NLP/opus-mt-en-fr           | \\~60M               | Traduction EN‚ÜíFR l√©g√®re | Rapide, petit, bon compromis  |\n",
    "| facebook/mbart-large-50-many-to-many | \\~610M              | Traduction multilingue  | Plus lourd, meilleure qualit√© |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b995a",
   "metadata": {},
   "source": [
    "| Crit√®re           | Mistral 7B            | LLaMA 3.2           |\n",
    "|-------------------|-----------------------|---------------------|\n",
    "| Taille            | ~4.1 Go               | ~2 Go               |\n",
    "| Param√®tres        | ~7 milliards          | ~7 milliards        |\n",
    "| RAM/VRAM requise  | ~7 Go (float16)       | ~4 Go (float16)     |\n",
    "| Vitesse           | Mod√©r√©e               | Plus rapide         |\n",
    "| Disponibilit√©     | Oui via Ollama        | Oui via Ollama      |\n",
    "| Usage principal   | Traduction, texte     | Chat, traduction    |\n",
    "| Facilit√© d‚Äôusage  | Moyenne               | Moyenne             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95cf507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch version : 2.7.1+cpu\n",
      "üì¶ GPU dispo : False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"‚úÖ PyTorch version :\", torch.__version__)\n",
    "print(\"üì¶ GPU dispo :\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1215cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\merwa\\Documents\\GitHub\\merwan.boudrias\\projet_trad\\venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "#Charger mod√®les Hugging Face\n",
    "model_1_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "model_2_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "\n",
    "tokenizer_1 = MarianTokenizer.from_pretrained(model_1_name)\n",
    "model_1 = MarianMTModel.from_pretrained(model_1_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer_2 = MBart50Tokenizer.from_pretrained(model_2_name)\n",
    "model_2 = MBartForConditionalGeneration.from_pretrained(model_2_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def translate_hf(model, tokenizer, texts, temperature=0.0):\n",
    "    device = next(model.parameters()).device\n",
    "    if \"mbart\" in tokenizer.name_or_path.lower():\n",
    "        tokenizer.src_lang = \"en_XX\"\n",
    "        tokenizer.tgt_lang = \"fr_XX\"\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"fr_XX\"],\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature if temperature > 0 else None,\n",
    "            num_beams=5 if temperature == 0 else 1,\n",
    "            max_length=128,\n",
    "        )\n",
    "    else:\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=temperature > 0,\n",
    "            temperature=temperature if temperature > 0 else None,\n",
    "            num_beams=5 if temperature == 0 else 1,\n",
    "            max_length=128,\n",
    "        )\n",
    "    translated = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return translated\n",
    "\n",
    "def translate_ollama(model_name, text, temperature=0.0):\n",
    "    \"\"\"\n",
    "    Traduction via Ollama avec variation du style selon la temp√©rature,\n",
    "    mais consigne claire : retourner uniquement la traduction, sans aucun commentaire.\n",
    "    \"\"\"\n",
    "\n",
    "    #Valeur par d√©faut de secours (s√©curit√©)\n",
    "    base_prompt = \"Translate the following English text to French.\"\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        base_prompt = \"Translate the following English text to French. Be precise and literal.\"\n",
    "    elif temperature <= 0.2:\n",
    "        base_prompt = \"Translate the following English text to French with natural phrasing.\"\n",
    "    elif temperature <= 0.5:\n",
    "        base_prompt = \"Translate the following English text to French with a balance between literal and creative style.\"\n",
    "    elif temperature <= 0.8:\n",
    "        base_prompt = \"Translate the following English text to French in a more creative and expressive way.\"\n",
    "    elif temperature <= 1.0:\n",
    "        base_prompt = \"Translate the following English text to French in a free, creative way, using expressive French.\"\n",
    "\n",
    "    #Ajout de la consigne stricte selon le mod√®le\n",
    "    if model_name.startswith(\"mistral\"):\n",
    "        base_prompt += \"\\nDo not include any explanation, greeting, or formatting. Just return the French translation as plain text only. Nothing else.\"\n",
    "    else:\n",
    "        base_prompt += \"\\nReturn ONLY the French translation. No explanations or formatting.\"\n",
    "\n",
    "    #Construction finale du prompt\n",
    "    prompt = f\"{base_prompt}\\n\\n{text}\"\n",
    "\n",
    "    command = [\n",
    "        OLLAMA_PATH,\n",
    "        \"run\",\n",
    "        model_name,\n",
    "    ]\n",
    "    try:\n",
    "        result = subprocess.run(command, input=prompt, capture_output=True, text=True, encoding=\"utf-8\", timeout=20)\n",
    "        if result.returncode != 0:\n",
    "            return f\"Error: {result.stderr.strip()}\"\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Exception: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd147fd",
   "metadata": {},
   "source": [
    "# 3) Utilisation de diff√©rentes temp√©ratures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b074b",
   "metadata": {},
   "source": [
    "## Explication des m√©triques\n",
    "\n",
    "### BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "**Qu‚Äôest-ce que c‚Äôest ?**  \n",
    "BLEU mesure la similarit√© entre la traduction automatique et une ou plusieurs traductions de r√©f√©rence humaines.\n",
    "\n",
    "**Comment √ßa marche ?**  \n",
    "Il compte la correspondance des n-grammes (groupes de mots) dans la traduction g√©n√©r√©e par rapport √† la r√©f√©rence.\n",
    "\n",
    "**Interpr√©tation :**\n",
    "\n",
    "- 0-10 : Qualit√© tr√®s faible, traduction presque incompr√©hensible ou hors sujet.  \n",
    "- 10-30 : Traduction compr√©hensible mais avec plusieurs erreurs.  \n",
    "- 30-50 : Bonne traduction, souvent utilisable, quelques erreurs mineures.  \n",
    "- >50 : Traduction de tr√®s bonne qualit√©, proche d‚Äôune traduction humaine.\n",
    "\n",
    "**Attention :**  \n",
    "BLEU ne capture pas toujours la fluidit√© ou la qualit√© s√©mantique parfaite.\n",
    "\n",
    "---\n",
    "\n",
    "### ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest common subsequence)\n",
    "\n",
    "**Qu‚Äôest-ce que c‚Äôest ?**  \n",
    "ROUGE-L mesure la qualit√© de la traduction en √©valuant la plus longue sous-s√©quence commune entre la traduction g√©n√©r√©e et la r√©f√©rence.\n",
    "\n",
    "**Comment √ßa marche ?**  \n",
    "Il √©value la structure globale, la coh√©rence et le contenu partag√©, prenant en compte la fluidit√© plus que BLEU.\n",
    "\n",
    "**Valeurs typiques :**\n",
    "\n",
    "- 0.0 √† 1.0 (ou 0% √† 100%), o√π 1 signifie correspondance parfaite.\n",
    "\n",
    "**Interpr√©tation :**\n",
    "\n",
    "- <0.3 : Faible correspondance, traduction de mauvaise qualit√©.  \n",
    "- 0.3 - 0.6 : Moyenne √† bonne qualit√©.  \n",
    "- >0.6 : Tr√®s bonne qualit√©, bonne fluidit√© et correspondance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262c8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traductions sur quelques phrases phrases du test WMT14...\n",
      "\n",
      "HF Helsinki - temp=0.0\n",
      "BLEU=31.01, ROUGE-L=0.586\n",
      "HF mBART - temp=0.0\n",
      "BLEU=26.65, ROUGE-L=0.549\n",
      "\n",
      "HF Helsinki - temp=0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU=27.54, ROUGE-L=0.563\n",
      "HF mBART - temp=0.2\n",
      "BLEU=29.38, ROUGE-L=0.563\n",
      "\n",
      "HF Helsinki - temp=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU=29.89, ROUGE-L=0.536\n",
      "HF mBART - temp=0.5\n",
      "BLEU=20.78, ROUGE-L=0.504\n",
      "\n",
      "HF Helsinki - temp=0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU=14.93, ROUGE-L=0.437\n",
      "HF mBART - temp=0.8\n",
      "BLEU=17.13, ROUGE-L=0.452\n",
      "\n",
      "HF Helsinki - temp=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU=19.43, ROUGE-L=0.441\n",
      "HF mBART - temp=1.0\n",
      "BLEU=10.67, ROUGE-L=0.390\n",
      "\n",
      "Ollama llama3.2 - temp=0.0\n",
      "BLEU=24.14, ROUGE-L=0.519\n",
      "\n",
      "Ollama llama3.2 - temp=0.2\n",
      "BLEU=20.26, ROUGE-L=0.490\n",
      "\n",
      "Ollama llama3.2 - temp=0.5\n",
      "BLEU=28.88, ROUGE-L=0.547\n",
      "\n",
      "Ollama llama3.2 - temp=0.8\n",
      "BLEU=21.39, ROUGE-L=0.436\n",
      "\n",
      "Ollama llama3.2 - temp=1.0\n",
      "BLEU=16.04, ROUGE-L=0.445\n",
      "\n",
      "Ollama mistral:7b - temp=0.0\n",
      "BLEU=21.43, ROUGE-L=0.458\n",
      "\n",
      "Ollama mistral:7b - temp=0.2\n",
      "BLEU=23.08, ROUGE-L=0.454\n",
      "\n",
      "Ollama mistral:7b - temp=0.5\n",
      "BLEU=25.70, ROUGE-L=0.512\n",
      "\n",
      "Ollama mistral:7b - temp=0.8\n",
      "BLEU=20.73, ROUGE-L=0.428\n",
      "\n",
      "Ollama mistral:7b - temp=1.0\n",
      "BLEU=20.69, ROUGE-L=0.488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#√âvaluation automatique BLEU & ROUGE\n",
    "def evaluate_translations(references, predictions):\n",
    "    #BLEU\n",
    "    bleu = corpus_bleu(predictions, [references])\n",
    "    \n",
    "    #ROUGE\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1':0, 'rouge2':0, 'rougeL':0}\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        for key in rouge_scores.keys():\n",
    "            rouge_scores[key] += scores[key].fmeasure\n",
    "    #Moyenne\n",
    "    n = len(references)\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key] /= n\n",
    "    return bleu.score, rouge_scores\n",
    "\n",
    "#Tester diff√©rents mod√®les avec temp√©ratures\n",
    "temperatures = [0.0, 0.2, 0.5, 0.8, 1.0]\n",
    "\n",
    "results = {\n",
    "    \"huggingface_helsinki\": {},\n",
    "    \"huggingface_mbart\": {},\n",
    "    \"ollama_llama3.2\": {},\n",
    "    \"ollama_mistral7b\": {},\n",
    "}\n",
    "\n",
    "print(\"Traductions sur quelques phrases phrases du test WMT14...\\n\")\n",
    "\n",
    "#Hugging Face\n",
    "for temp in temperatures:\n",
    "    print(f\"HF Helsinki - temp={temp}\")\n",
    "    pred1 = translate_hf(model_1, tokenizer_1, sources, temperature=temp)\n",
    "    results[\"huggingface_helsinki\"][temp] = pred1\n",
    "    bleu_score, rouge_scores = evaluate_translations(references, pred1)\n",
    "    print(f\"BLEU={bleu_score:.2f}, ROUGE-L={rouge_scores['rougeL']:.3f}\")\n",
    "\n",
    "    print(f\"HF mBART - temp={temp}\")\n",
    "    pred2 = translate_hf(model_2, tokenizer_2, sources, temperature=temp)\n",
    "    results[\"huggingface_mbart\"][temp] = pred2\n",
    "    bleu_score, rouge_scores = evaluate_translations(references, pred2)\n",
    "    print(f\"BLEU={bleu_score:.2f}, ROUGE-L={rouge_scores['rougeL']:.3f}\\n\")\n",
    "\n",
    "#Ollama avec temp√©ratures simul√©es dans le prompt\n",
    "for temp in temperatures:\n",
    "    print(f\"Ollama llama3.2 - temp={temp}\")\n",
    "    preds_llama = [translate_ollama(\"llama3.2:latest\", text, temperature=temp) for text in sources]\n",
    "    results[\"ollama_llama3.2\"][temp] = preds_llama\n",
    "    bleu_score, rouge_scores = evaluate_translations(references, preds_llama)\n",
    "    print(f\"BLEU={bleu_score:.2f}, ROUGE-L={rouge_scores['rougeL']:.3f}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"Ollama mistral:7b - temp={temp}\")\n",
    "    preds_mistral = [translate_ollama(\"mistral:7b\", text, temperature=temp) for text in sources]\n",
    "    results[\"ollama_mistral7b\"][temp] = preds_mistral\n",
    "    bleu_score, rouge_scores = evaluate_translations(references, preds_mistral)\n",
    "    print(f\"BLEU={bleu_score:.2f}, ROUGE-L={rouge_scores['rougeL']:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2151064",
   "metadata": {},
   "source": [
    "# 4) M√©triques d'√©valuations (BLEU + ROUGE-L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373686af",
   "metadata": {},
   "source": [
    "## R√©sultats de l'√©valuation sur 10 phrases du jeu de test WMT14 (EN ‚Üí FR)\n",
    "\n",
    "| Mod√®le                | Temp√©rature | BLEU  | ROUGE-L |\n",
    "| --------------------- | ----------- | ----- | ------- |\n",
    "| **HF Helsinki**       | 0.0         | 31.01 | 0.586   |\n",
    "|                       | 0.2         | 27.54 | 0.563   |\n",
    "|                       | 0.5         | 29.89 | 0.536   |\n",
    "|                       | 0.8         | 14.93 | 0.437   |\n",
    "|                       | 1.0         | 19.43 | 0.441   |\n",
    "| **HF mBART**          | 0.0         | 26.65 | 0.549   |\n",
    "|                       | 0.2         | 29.38 | 0.563   |\n",
    "|                       | 0.5         | 20.78 | 0.504   |\n",
    "|                       | 0.8         | 17.13 | 0.452   |\n",
    "|                       | 1.0         | 10.67 | 0.390   |\n",
    "| **Ollama llama3.2**   | 0.0         | 24.14 | 0.519   |\n",
    "|                       | 0.2         | 20.26 | 0.490   |\n",
    "|                       | 0.5         | 28.88 | 0.547   |\n",
    "|                       | 0.8         | 21.39 | 0.436   |\n",
    "|                       | 1.0         | 16.04 | 0.445   |\n",
    "| **Ollama mistral:7b** | 0.0         | 21.43 | 0.458   |\n",
    "|                       | 0.2         | 23.08 | 0.454   |\n",
    "|                       | 0.5         | 25.70 | 0.512   |\n",
    "|                       | 0.8         | 20.73 | 0.428   |\n",
    "|                       | 1.0         | 20.69 | 0.488   |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Explication des m√©triques\n",
    "\n",
    "### üîµ BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "**D√©finition :**  \n",
    "BLEU mesure la similarit√© entre la traduction automatique et une ou plusieurs traductions humaines de r√©f√©rence.\n",
    "\n",
    "**Comment √ßa fonctionne ?**  \n",
    "Il compare des groupes de mots (n-grammes) entre la sortie du mod√®le et la r√©f√©rence, en p√©nalisant les traductions trop courtes.\n",
    "\n",
    "**Interpr√©tation des scores :**\n",
    "\n",
    "| Score BLEU | Qualit√© de traduction                   |\n",
    "|------------|------------------------------------------|\n",
    "| 0‚Äì10       | Tr√®s faible (souvent incompr√©hensible)  |\n",
    "| 10‚Äì30      | Moyen (compr√©hensible avec erreurs)      |\n",
    "| 30‚Äì50      | Bon (utilisable, erreurs mineures)       |\n",
    "| >50        | Excellent (proche d‚Äôune traduction humaine) |\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ ROUGE-L (Longest Common Subsequence)\n",
    "\n",
    "**D√©finition :**  \n",
    "ROUGE-L mesure la qualit√© de la traduction en √©valuant la **plus longue sous-s√©quence commune** entre la pr√©diction et la r√©f√©rence.\n",
    "\n",
    "**Avantage :**  \n",
    "Il capte la **structure globale et la fluidit√©** du texte (meilleur compl√©ment au BLEU).\n",
    "\n",
    "**Interpr√©tation des scores :**\n",
    "\n",
    "| Score ROUGE-L | Qualit√© de traduction                    |\n",
    "|---------------|-------------------------------------------|\n",
    "| <0.3          | Faible qualit√©, structure incoh√©rente     |\n",
    "| 0.3 ‚Äì 0.6     | Moyenne √† bonne qualit√©                   |\n",
    "| >0.6          | Tr√®s bonne qualit√© et bonne coh√©rence     |\n",
    "\n",
    "---\n",
    "\n",
    "üëâ **Remarque :**  \n",
    "Les scores peuvent varier selon la longueur des textes, la complexit√© s√©mantiqu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f226a",
   "metadata": {},
   "source": [
    "# 5) Gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76b14ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#D√©mo Gradio corrig√©e\n",
    "def demo_translate(text, model_choice, temperature):\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"Veuillez saisir un texte !\"  \n",
    "    if model_choice == \"Helsinki-NLP/opus-mt-en-fr\":\n",
    "        translation = translate_hf(model_1, tokenizer_1, [text], temperature=temperature)[0]\n",
    "    elif model_choice == \"facebook/mbart-large-50-many-to-many-mmt\":\n",
    "        translation = translate_hf(model_2, tokenizer_2, [text], temperature=temperature)[0]\n",
    "    elif model_choice == \"llama3.2\":\n",
    "        translation = translate_ollama(\"llama3.2:latest\", text, temperature=temperature)\n",
    "    elif model_choice == \"mistral:7b\":\n",
    "        translation = translate_ollama(\"mistral:7b\", text, temperature=temperature)\n",
    "    else:\n",
    "        translation = \"Mod√®le inconnu\"\n",
    "    return translation\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=demo_translate,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=3, label=\"Texte anglais √† traduire\"),\n",
    "        gr.Dropdown(\n",
    "            choices=[\"Helsinki-NLP/opus-mt-en-fr\", \"facebook/mbart-large-50-many-to-many-mmt\", \"llama3.2\", \"mistral:7b\"],\n",
    "            label=\"Mod√®le\"\n",
    "        ),\n",
    "        gr.Dropdown(\n",
    "            choices=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            label=\"Temp√©rature\",\n",
    "            value=0\n",
    "        )\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"D√©mo traduction anglais‚Üífran√ßais\",\n",
    "    description=\"Testez la traduction en temps r√©el avec Hugging Face et Ollama (temp√©rature simul√©e dans Ollama via prompt)\",\n",
    "    \n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
